# -*- coding: utf-8 -*-
"""cell_network_analysis_forecasting

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zoBAkDcjkN0XHOZFPbB4LDy3Ov4V9c6z

#importing libraries
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
import numpy as np
import warnings
warnings.filterwarnings('ignore')

"""# mounting Drive"""

from google.colab import drive
drive.mount('/content/drive')

"""# reading the csv files"""

data_dir = '/content/drive/MyDrive/task 1/'
data_list = []

# Load each file into a DataFrame and store it in the list
for day in range(1, 16):
    file_path = os.path.join(data_dir, f'features_day{day}.csv')
    df = pd.read_csv(file_path)
    data_list.append(df)

#concatenate all DataFrames into a single DataFrame
all_data = pd.concat(data_list, keys=range(1, 16), names=['Day'])

df = pd.DataFrame(all_data)
df

"""#**EDA**

Initial Data Inspection
"""

# Check the first few rows of the combined data
print(df.head())

"""Data Type Check"""

print(df.info())

print(df.describe())

"""check null values"""

# Check for missing values
print(df.isnull().sum())

"""check duplicates"""

# Check for duplicate rows
duplicates = df[df.duplicated()]

# Display the number of duplicate rows
print(f"Number of duplicate rows: {len(duplicates)}")

# Display the duplicate rows if any
print(duplicates.head())

"""drop duplicates"""

# Remove duplicate rows
df = df.drop_duplicates()

# Verify the removal
print(f"Number of rows after removing duplicates: {len(df)}")

"""apply featur engineering"""

#apply feature engineering (eucleadian distance on cell_x ,cell_y ,  cell_z )
# Calculate the Euclidean distance
df['euclidean_distance'] = np.sqrt(df['cell_x']**2 + df['cell_y']**2 + df['cell_z']**2)
df

"""Column Arrangement"""

# List of columns in the original order
cols = list(df.columns)

# Remove feature_11 and feature_12 from the current position
cols.remove('feature_11')
cols.remove('feature_12')

# Add feature_11 and feature_12 at the end
cols += ['feature_11', 'feature_12']

# Rearrange the DataFrame with the new column order
df = df[cols]

# Display the rearranged DataFrame
print(df.head())

# Check data types of each column
print(df.dtypes)

"""check the outliers"""

from itertools import count
Q1 = df.select_dtypes(include='float64').quantile(0.25)
Q3 = df.select_dtypes(include='float64').quantile(0.75)
IQR = Q3 - Q1
outliers = ((df.select_dtypes(include='float64') < (Q1 - 1.5 * IQR)) | (df.select_dtypes(include='float64') > (Q3 + 1.5 * IQR)))

plt.figure(figsize=(15, 10))
sns.boxplot(data=df.select_dtypes(include='float64'))
plt.xticks(rotation=90)
plt.show()

sns.pairplot(df.select_dtypes(include='float64'))
plt.show()

"""# identify the relationship between feature & target"""

# Compute correlation matrix between feature and targets (feature 11 , feature 12)
corr_matrix = df.corr()

# Plot heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix[['feature_11', 'feature_12']], annot=True, cmap='coolwarm')
plt.title('Correlation Matrix with Target Features')
plt.show()

# Scatter plots
features = ['feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5',
            'feature_6', 'feature_7', 'feature_8', 'feature_9', 'feature_10',
            'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17',
            'feature_18', 'feature_19' , 'euclidean_distance']

for feature in features:
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    sns.scatterplot(data=df, x=feature, y='feature_11')
    plt.title(f'{feature} vs. feature_11')

    plt.subplot(1, 2, 2)
    sns.scatterplot(data=df, x=feature, y='feature_12')
    plt.title(f'{feature} vs. feature_12')

    plt.tight_layout()
    plt.show()

# feature importance
from sklearn.ensemble import RandomForestRegressor

# Prepare data for modeling
X = df[features]
y = df[['feature_11', 'feature_12']]

# Train model for feature importance
rf = RandomForestRegressor(n_estimators=100)
rf.fit(X, y['feature_11'])  # Example for feature_11

# Feature importance
importance = rf.feature_importances_
feature_importance = pd.DataFrame({'Feature': features, 'Importance': importance})
feature_importance = feature_importance.sort_values(by='Importance', ascending=False)

print(feature_importance)

"""check the distribution of data"""

#check the distribution of the data using histogram
df.hist(bins=50, figsize=(20,15))
plt.show()

"""#apply scalling (standarization)"""

#applying data standarization
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df[features] = scaler.fit_transform(df[features])
df

!pip install category_encoders

"""apply one hot encoding

"""

# Apply label encoding to 'feature_11'
df['feature_11'] = df['feature_11'].astype(int)

# Check the first 50 rows to confirm
print(df)

#apply linear regression model to predict feature_11 and feature_12

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    df[features], df[['feature_11', 'feature_12']], test_size=0.2, random_state=42
)

# Create and train the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
accuracy = model.score(X_test, y_test)

print("Mean Squared Error:", mse)
print("R-squared:", r2)
print("accuracy:",accuracy)